{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM9GaP1C6Y6inYsTOmicUJA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ryn1221/cbir-code/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxdwQAGnZ7xu",
        "outputId": "b1929921-8156-409e-eef9-fb28df1bd84d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "DB_dir = 'database'\n",
        "DB_csv = 'data.csv'\n",
        "\n",
        "\n",
        "class Database(object):\n",
        "\n",
        "  def __init__(self):\n",
        "    self._gen_csv()\n",
        "    self.data = pd.read_csv(DB_csv)\n",
        "    self.classes = set(self.data[\"cls\"])\n",
        "\n",
        "  def _gen_csv(self):\n",
        "    if os.path.exists(DB_csv):\n",
        "      return\n",
        "    with open(DB_csv, 'w', encoding='UTF-8') as f:\n",
        "      f.write(\"img,cls\")\n",
        "      for root, _, files in os.walk(DB_dir, topdown=False):\n",
        "        cls = root.split('/')[-1]\n",
        "        for name in files:\n",
        "          if not name.endswith('.jpg'):\n",
        "            continue\n",
        "          img = os.path.join(root, name)\n",
        "          f.write(\"\\n{},{}\".format(img, cls))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def get_class(self):\n",
        "    return self.classes\n",
        "\n",
        "  def get_data(self):\n",
        "    return self.data\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  db = Database()\n",
        "  data = db.get_data()\n",
        "  classes = db.get_class()\n",
        "\n",
        "  print(\"DB length:\", len(db))\n",
        "  print(classes)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DB length: 0\n",
            "set()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "from evaluate import evaluate_class\n",
        "from DB import Database\n",
        "\n",
        "from skimage.feature import hog\n",
        "from skimage import color\n",
        "\n",
        "from six.moves import cPickle\n",
        "import numpy as np\n",
        "import scipy.misc\n",
        "import os\n",
        "\n",
        "n_bin    = 10\n",
        "n_slice  = 6\n",
        "n_orient = 8\n",
        "p_p_c    = (2, 2)\n",
        "c_p_b    = (1, 1)\n",
        "h_type   = 'region'\n",
        "d_type   = 'd1'\n",
        "\n",
        "depth    = 5\n",
        "\n",
        "''' MMAP\n",
        "     depth\n",
        "      depthNone, HOG-region-n_bin10-n_slice6-n_orient8-ppc(2, 2)-cpb(1, 1), distance=d1, MMAP 0.155887235348\n",
        "      depth100,  HOG-region-n_bin10-n_slice6-n_orient8-ppc(2, 2)-cpb(1, 1), distance=d1, MMAP 0.261149622088\n",
        "      depth30,   HOG-region-n_bin10-n_slice6-n_orient8-ppc(2, 2)-cpb(1, 1), distance=d1, MMAP 0.371054105819\n",
        "      depth10,   HOG-region-n_bin10-n_slice6-n_orient8-ppc(2, 2)-cpb(1, 1), distance=d1, MMAP 0.449627835097\n",
        "      depth5,    HOG-region-n_bin10-n_slice6-n_orient8-ppc(2, 2)-cpb(1, 1), distance=d1, MMAP 0.465333333333\n",
        "      depth3,    HOG-region-n_bin10-n_slice6-n_orient8-ppc(2, 2)-cpb(1, 1), distance=d1, MMAP 0.463833333333\n",
        "      depth1,    HOG-region-n_bin10-n_slice6-n_orient8-ppc(2, 2)-cpb(1, 1), distance=d1, MMAP 0.398\n",
        "\n",
        "      (exps below use depth=None)\n",
        "\n",
        "     ppc & cpb\n",
        "      HOG-global-n_bin10-n_orient8-ppc(2, 2)-cpb(1, 1), distance=d1, MMAP 0.105569494513\n",
        "      HOG-global-n_bin10-n_orient8-ppc(32, 32)-cpb(1, 1), distance=d1, MMAP 0.0945343258574\n",
        "      HOG-global-n_bin10-n_orient8-ppc(8, 8)-cpb(3, 3), distance=d1, MMAP 0.0782408187317\n",
        "\n",
        "     h_type\n",
        "      HOG-global-n_bin100-n_orient8-ppc(32, 32)-cpb(1, 1), distance=d1, MMAP 0.0990826443803\n",
        "      HOG-region-n_bin100-n_slice4-n_orient8-ppc(32, 32)-cpb(1, 1), distance=d1, MMAP 0.131164310773\n",
        "\n",
        "     n_orient\n",
        "      HOG-global-n_bin10-n_orient8-ppc(2, 2)-cpb(1, 1), distance=d1, MMAP 0.105569494513\n",
        "      HOG-region-n_bin10-n_slice4-n_orient18-ppc(2, 2)-cpb(1, 1), distance=d1, MMAP 0.14941454752\n",
        "\n",
        "     n_bin\n",
        "      HOG-region-n_bin5-n_slice4-n_orient8-ppc(32, 32)-cpb(1, 1), distance=d1, MMAP 0.140448910465\n",
        "      HOG-region-n_bin10-n_slice4-n_orient8-ppc(32, 32)-cpb(1, 1), distance=d1, MMAP 0.144675311048\n",
        "      HOG-region-n_bin20-n_slice4-n_orient8-ppc(32, 32)-cpb(1, 1), distance=d1, MMAP 0.1429074023\n",
        "      HOG-region-n_bin100-n_slice4-n_orient8-ppc(32, 32)-cpb(1, 1), distance=d1, MMAP 0.131164310773\n",
        "\n",
        "     n_slice\n",
        "      HOG-region-n_bin10-n_slice2-n_orient8-ppc(2, 2)-cpb(1, 1), distance=d1, MMAP 0.116513458785\n",
        "      HOG-region-n_bin10-n_slice4-n_orient8-ppc(2, 2)-cpb(1, 1), distance=d1, MMAP 0.151557545391\n",
        "      HOG-region-n_bin10-n_slice6-n_orient8-ppc(2, 2)-cpb(1, 1), distance=d1, MMAP 0.155887235348\n",
        "      HOG-region-n_bin10-n_slice8-n_orient8-ppc(2, 2)-cpb(1, 1), distance=d1, MMAP 0.15347983005\n",
        "'''\n",
        "\n",
        "# cache dir\n",
        "cache_dir = 'cache'\n",
        "if not os.path.exists(cache_dir):\n",
        "  os.makedirs(cache_dir)\n",
        "\n",
        "\n",
        "class HOG(object):\n",
        "\n",
        "  def histogram(self, input, n_bin=n_bin, type=h_type, n_slice=n_slice, normalize=True):\n",
        "    ''' count img histogram\n",
        "  \n",
        "      arguments\n",
        "        input    : a path to a image or a numpy.ndarray\n",
        "        n_bin    : number of bins of histogram\n",
        "        type     : 'global' means count the histogram for whole image\n",
        "                   'region' means count the histogram for regions in images, then concatanate all of them\n",
        "        n_slice  : work when type equals to 'region', height & width will equally sliced into N slices\n",
        "        normalize: normalize output histogram\n",
        "  \n",
        "      return\n",
        "        type == 'global'\n",
        "          a numpy array with size n_bin\n",
        "        type == 'region'\n",
        "          a numpy array with size n_bin * n_slice * n_slice\n",
        "    '''\n",
        "    if isinstance(input, np.ndarray):  # examinate input type\n",
        "      img = input.copy()\n",
        "    else:\n",
        "      img = scipy.misc.imread(input, mode='RGB')\n",
        "    height, width, channel = img.shape\n",
        "  \n",
        "    if type == 'global':\n",
        "      hist = self._HOG(img, n_bin)\n",
        "  \n",
        "    elif type == 'region':\n",
        "      hist = np.zeros((n_slice, n_slice, n_bin))\n",
        "      h_silce = np.around(np.linspace(0, height, n_slice+1, endpoint=True)).astype(int)\n",
        "      w_slice = np.around(np.linspace(0, width, n_slice+1, endpoint=True)).astype(int)\n",
        "  \n",
        "      for hs in range(len(h_silce)-1):\n",
        "        for ws in range(len(w_slice)-1):\n",
        "          img_r = img[h_silce[hs]:h_silce[hs+1], w_slice[ws]:w_slice[ws+1]]  # slice img to regions\n",
        "          hist[hs][ws] = self._HOG(img_r, n_bin)\n",
        "  \n",
        "    if normalize:\n",
        "      hist /= np.sum(hist)\n",
        "  \n",
        "    return hist.flatten()\n",
        "\n",
        "  def _HOG(self, img, n_bin, normalize=True):\n",
        "    image = color.rgb2gray(img)\n",
        "    fd = hog(image, orientations=n_orient, pixels_per_cell=p_p_c, cells_per_block=c_p_b)\n",
        "    bins = np.linspace(0, np.max(fd), n_bin+1, endpoint=True)\n",
        "    hist, _ = np.histogram(fd, bins=bins)\n",
        "  \n",
        "    if normalize:\n",
        "      hist = np.array(hist) / np.sum(hist)\n",
        "  \n",
        "    return hist\n",
        "\n",
        "  def make_samples(self, db, verbose=True):\n",
        "    if h_type == 'global':\n",
        "      sample_cache = \"HOG-{}-n_bin{}-n_orient{}-ppc{}-cpb{}\".format(h_type, n_bin, n_orient, p_p_c, c_p_b)\n",
        "    elif h_type == 'region':\n",
        "      sample_cache = \"HOG-{}-n_bin{}-n_slice{}-n_orient{}-ppc{}-cpb{}\".format(h_type, n_bin, n_slice, n_orient, p_p_c, c_p_b)\n",
        "  \n",
        "    try:\n",
        "      samples = cPickle.load(open(os.path.join(cache_dir, sample_cache), \"rb\", True))\n",
        "      for sample in samples:\n",
        "        sample['hist'] /= np.sum(sample['hist'])  # normalize\n",
        "      if verbose:\n",
        "        print(\"Using cache..., config=%s, distance=%s, depth=%s\" % (sample_cache, d_type, depth))\n",
        "    except:\n",
        "      if verbose:\n",
        "        print(\"Counting histogram..., config=%s, distance=%s, depth=%s\" % (sample_cache, d_type, depth))\n",
        "\n",
        "      samples = []\n",
        "      data = db.get_data()\n",
        "      for d in data.itertuples():\n",
        "        d_img, d_cls = getattr(d, \"img\"), getattr(d, \"cls\")\n",
        "        d_hist = self.histogram(d_img, type=h_type, n_slice=n_slice)\n",
        "        samples.append({\n",
        "                        'img':  d_img, \n",
        "                        'cls':  d_cls, \n",
        "                        'hist': d_hist\n",
        "                      })\n",
        "      cPickle.dump(samples, open(os.path.join(cache_dir, sample_cache), \"wb\", True))\n",
        "\n",
        "    return samples\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  db = Database()\n",
        "\n",
        "  # evaluate database\n",
        "  APs = evaluate_class(db, f_class=HOG, d_type=d_type, depth=depth)\n",
        "  cls_MAPs = []\n",
        "  for cls, cls_APs in APs.items():\n",
        "    MAP = np.mean(cls_APs)\n",
        "    print(\"Class {}, MAP {}\".format(cls, MAP))\n",
        "    cls_MAPs.append(MAP)\n",
        "  print(\"MMAP\", np.mean(cls_MAPs))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "praqZk_ZHIfI",
        "outputId": "8dbda68b-5b25-4a1c-d93a-f401e7e7a96f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-7b92becfd688>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mevaluate\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mevaluate_class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mDB\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDatabase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'evaluate'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "from evaluate import distance, evaluate_class\n",
        "from DB import Database\n",
        "\n",
        "from six.moves import cPickle\n",
        "import numpy as np\n",
        "import scipy.misc\n",
        "import itertools\n",
        "import os\n",
        "\n",
        "\n",
        "# configs for histogram\n",
        "n_bin   = 12        # histogram bins\n",
        "n_slice = 3         # slice image\n",
        "h_type  = 'region'  # global or region\n",
        "d_type  = 'd1'      # distance type\n",
        "\n",
        "depth   = 3         # retrieved depth, set to None will count the ap for whole database\n",
        "\n",
        "''' MMAP\n",
        "     depth\n",
        "      depthNone, region,bin12,slice3, distance=d1, MMAP 0.273745840034\n",
        "      depth100,  region,bin12,slice3, distance=d1, MMAP 0.406007856783\n",
        "      depth30,   region,bin12,slice3, distance=d1, MMAP 0.516738512679\n",
        "      depth10,   region,bin12,slice3, distance=d1, MMAP 0.614047666604\n",
        "      depth5,    region,bin12,slice3, distance=d1, MMAP 0.650125\n",
        "      depth3,    region,bin12,slice3, distance=d1, MMAP 0.657166666667\n",
        "      depth1,    region,bin12,slice3, distance=d1, MMAP 0.62\n",
        "\n",
        "     (exps below use depth=None)\n",
        "     \n",
        "     d_type\n",
        "      global,bin6,d1,MMAP 0.242345913685\n",
        "      global,bin6,cosine,MMAP 0.184176505586\n",
        "\n",
        "     n_bin\n",
        "      region,bin10,slice4,d1,MMAP 0.269872790396\n",
        "      region,bin12,slice4,d1,MMAP 0.271520862017\n",
        "\n",
        "      region,bin6,slcie3,d1,MMAP 0.262819311357\n",
        "      region,bin12,slice3,d1,MMAP 0.273745840034\n",
        "\n",
        "     n_slice\n",
        "      region,bin12,slice2,d1,MMAP 0.266076627332\n",
        "      region,bin12,slice3,d1,MMAP 0.273745840034\n",
        "      region,bin12,slice4,d1,MMAP 0.271520862017\n",
        "      region,bin14,slice3,d1,MMAP 0.272386552594\n",
        "      region,bin14,slice5,d1,MMAP 0.266877181379\n",
        "      region,bin16,slice3,d1,MMAP 0.273716788003\n",
        "      region,bin16,slice4,d1,MMAP 0.272221031804\n",
        "      region,bin16,slice8,d1,MMAP 0.253823360098\n",
        "\n",
        "     h_type\n",
        "      region,bin4,slice2,d1,MMAP 0.23358615622\n",
        "      global,bin4,d1,MMAP 0.229125435746\n",
        "'''\n",
        "\n",
        "# cache dir\n",
        "cache_dir = 'cache'\n",
        "if not os.path.exists(cache_dir):\n",
        "  os.makedirs(cache_dir)\n",
        "\n",
        "\n",
        "class Color(object):\n",
        "\n",
        "  def histogram(self, input, n_bin=n_bin, type=h_type, n_slice=n_slice, normalize=True):\n",
        "    ''' count img color histogram\n",
        "  \n",
        "      arguments\n",
        "        input    : a path to a image or a numpy.ndarray\n",
        "        n_bin    : number of bins for each channel\n",
        "        type     : 'global' means count the histogram for whole image\n",
        "                   'region' means count the histogram for regions in images, then concatanate all of them\n",
        "        n_slice  : work when type equals to 'region', height & width will equally sliced into N slices\n",
        "        normalize: normalize output histogram\n",
        "  \n",
        "      return\n",
        "        type == 'global'\n",
        "          a numpy array with size n_bin ** channel\n",
        "        type == 'region'\n",
        "          a numpy array with size n_slice * n_slice * (n_bin ** channel)\n",
        "    '''\n",
        "    if isinstance(input, np.ndarray):  # examinate input type\n",
        "      img = input.copy()\n",
        "    else:\n",
        "      img = scipy.misc.imread(input, mode='RGB')\n",
        "    height, width, channel = img.shape\n",
        "    bins = np.linspace(0, 256, n_bin+1, endpoint=True)  # slice bins equally for each channel\n",
        "  \n",
        "    if type == 'global':\n",
        "      hist = self._count_hist(img, n_bin, bins, channel)\n",
        "  \n",
        "    elif type == 'region':\n",
        "      hist = np.zeros((n_slice, n_slice, n_bin ** channel))\n",
        "      h_silce = np.around(np.linspace(0, height, n_slice+1, endpoint=True)).astype(int)\n",
        "      w_slice = np.around(np.linspace(0, width, n_slice+1, endpoint=True)).astype(int)\n",
        "  \n",
        "      for hs in range(len(h_silce)-1):\n",
        "        for ws in range(len(w_slice)-1):\n",
        "          img_r = img[h_silce[hs]:h_silce[hs+1], w_slice[ws]:w_slice[ws+1]]  # slice img to regions\n",
        "          hist[hs][ws] = self._count_hist(img_r, n_bin, bins, channel)\n",
        "  \n",
        "    if normalize:\n",
        "      hist /= np.sum(hist)\n",
        "  \n",
        "    return hist.flatten()\n",
        "  \n",
        "  \n",
        "  def _count_hist(self, input, n_bin, bins, channel):\n",
        "    img = input.copy()\n",
        "    bins_idx = {key: idx for idx, key in enumerate(itertools.product(np.arange(n_bin), repeat=channel))}  # permutation of bins\n",
        "    hist = np.zeros(n_bin ** channel)\n",
        "  \n",
        "    # cluster every pixels\n",
        "    for idx in range(len(bins)-1):\n",
        "      img[(input >= bins[idx]) & (input < bins[idx+1])] = idx\n",
        "    # add pixels into bins\n",
        "    height, width, _ = img.shape\n",
        "    for h in range(height):\n",
        "      for w in range(width):\n",
        "        b_idx = bins_idx[tuple(img[h,w])]\n",
        "        hist[b_idx] += 1\n",
        "  \n",
        "    return hist\n",
        "  \n",
        "  \n",
        "  def make_samples(self, db, verbose=True):\n",
        "    if h_type == 'global':\n",
        "      sample_cache = \"histogram_cache-{}-n_bin{}\".format(h_type, n_bin)\n",
        "    elif h_type == 'region':\n",
        "      sample_cache = \"histogram_cache-{}-n_bin{}-n_slice{}\".format(h_type, n_bin, n_slice)\n",
        "    \n",
        "    try:\n",
        "      samples = cPickle.load(open(os.path.join(cache_dir, sample_cache), \"rb\", True))\n",
        "      if verbose:\n",
        "        print(\"Using cache..., config=%s, distance=%s, depth=%s\" % (sample_cache, d_type, depth))\n",
        "    except:\n",
        "      if verbose:\n",
        "        print(\"Counting histogram..., config=%s, distance=%s, depth=%s\" % (sample_cache, d_type, depth))\n",
        "      samples = []\n",
        "      data = db.get_data()\n",
        "      for d in data.itertuples():\n",
        "        d_img, d_cls = getattr(d, \"img\"), getattr(d, \"cls\")\n",
        "        d_hist = self.histogram(d_img, type=h_type, n_bin=n_bin, n_slice=n_slice)\n",
        "        samples.append({\n",
        "                        'img':  d_img, \n",
        "                        'cls':  d_cls, \n",
        "                        'hist': d_hist\n",
        "                      })\n",
        "      cPickle.dump(samples, open(os.path.join(cache_dir, sample_cache), \"wb\", True))\n",
        "  \n",
        "    return samples\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  db = Database()\n",
        "  data = db.get_data()\n",
        "  color = Color()\n",
        "\n",
        "  # test normalize\n",
        "  hist = color.histogram(data.ix[0,0], type='global')\n",
        "  assert hist.sum() - 1 < 1e-9, \"normalize false\"\n",
        "\n",
        "  # test histogram bins\n",
        "  def sigmoid(z):\n",
        "    a = 1.0 / (1.0 + np.exp(-1. * z))\n",
        "    return a\n",
        "  np.random.seed(0)\n",
        "  IMG = sigmoid(np.random.randn(2,2,3)) * 255\n",
        "  IMG = IMG.astype(int)\n",
        "  hist = color.histogram(IMG, type='global', n_bin=4)\n",
        "  assert np.equal(np.where(hist > 0)[0], np.array([37, 43, 58, 61])).all(), \"global histogram implement failed\"\n",
        "  hist = color.histogram(IMG, type='region', n_bin=4, n_slice=2)\n",
        "  assert np.equal(np.where(hist > 0)[0], np.array([58, 125, 165, 235])).all(), \"region histogram implement failed\"\n",
        "\n",
        "  # examinate distance\n",
        "  np.random.seed(1)\n",
        "  IMG = sigmoid(np.random.randn(4,4,3)) * 255\n",
        "  IMG = IMG.astype(int)\n",
        "  hist = color.histogram(IMG, type='region', n_bin=4, n_slice=2)\n",
        "  IMG2 = sigmoid(np.random.randn(4,4,3)) * 255\n",
        "  IMG2 = IMG2.astype(int)\n",
        "  hist2 = color.histogram(IMG2, type='region', n_bin=4, n_slice=2)\n",
        "  assert distance(hist, hist2, d_type='d1') == 2, \"d1 implement failed\"\n",
        "  assert distance(hist, hist2, d_type='d2-norm') == 2, \"d2 implement failed\"\n",
        "\n",
        "  # evaluate database\n",
        "  APs = evaluate_class(db, f_class=Color, d_type=d_type, depth=depth)\n",
        "  cls_MAPs = []\n",
        "  for cls, cls_APs in APs.items():\n",
        "    MAP = np.mean(cls_APs)\n",
        "    print(\"Class {}, MAP {}\".format(cls, MAP))\n",
        "    cls_MAPs.append(MAP)\n",
        "  print(\"MMAP\", np.mean(cls_MAPs))\n"
      ],
      "metadata": {
        "id": "LJEGZMQpHOAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "from evaluate import evaluate_class\n",
        "from DB import Database\n",
        "\n",
        "from skimage.feature import daisy\n",
        "from skimage import color\n",
        "\n",
        "from six.moves import cPickle\n",
        "import numpy as np\n",
        "import scipy.misc\n",
        "import math\n",
        "\n",
        "import os\n",
        "\n",
        "\n",
        "n_slice    = 2\n",
        "n_orient   = 8\n",
        "step       = 10\n",
        "radius     = 30\n",
        "rings      = 2\n",
        "histograms = 6\n",
        "h_type     = 'region'\n",
        "d_type     = 'd1'\n",
        "\n",
        "depth      = 3\n",
        "\n",
        "R = (rings * histograms + 1) * n_orient\n",
        "\n",
        "''' MMAP\n",
        "     depth\n",
        "      depthNone, daisy-region-n_slice2-n_orient8-step10-radius30-rings2-histograms6, distance=d1, MMAP 0.162806083971\n",
        "      depth100,  daisy-region-n_slice2-n_orient8-step10-radius30-rings2-histograms6, distance=d1, MMAP 0.269333190731\n",
        "      depth30,   daisy-region-n_slice2-n_orient8-step10-radius30-rings2-histograms6, distance=d1, MMAP 0.388199474789\n",
        "      depth10,   daisy-region-n_slice2-n_orient8-step10-radius30-rings2-histograms6, distance=d1, MMAP 0.468182738095\n",
        "      depth5,    daisy-region-n_slice2-n_orient8-step10-radius30-rings2-histograms6, distance=d1, MMAP 0.497688888889\n",
        "      depth3,    daisy-region-n_slice2-n_orient8-step10-radius30-rings2-histograms6, distance=d1, MMAP 0.499833333333\n",
        "      depth1,    daisy-region-n_slice2-n_orient8-step10-radius30-rings2-histograms6, distance=d1, MMAP 0.448\n",
        "\n",
        "      (exps below use depth=None)\n",
        "\n",
        "     d_type\n",
        "      daisy-global-n_orient8-step180-radius58-rings2-histograms6, distance=d1, MMAP 0.101883969577\n",
        "      daisy-global-n_orient8-step180-radius58-rings2-histograms6, distance=cosine, MMAP 0.104779921854\n",
        "\n",
        "     h_type\n",
        "      daisy-global-n_orient8-step10-radius30-rings2-histograms6, distance=d1, MMAP 0.157738278588\n",
        "      daisy-region-n_slice2-n_orient8-step10-radius30-rings2-histograms6, distance=d1, MMAP 0.162806083971\n",
        "'''\n",
        "\n",
        "# cache dir\n",
        "cache_dir = 'cache'\n",
        "if not os.path.exists(cache_dir):\n",
        "  os.makedirs(cache_dir)\n",
        "\n",
        "\n",
        "class Daisy(object):\n",
        "\n",
        "  def histogram(self, input, type=h_type, n_slice=n_slice, normalize=True):\n",
        "    ''' count img histogram\n",
        "  \n",
        "      arguments\n",
        "        input    : a path to a image or a numpy.ndarray\n",
        "        type     : 'global' means count the histogram for whole image\n",
        "                   'region' means count the histogram for regions in images, then concatanate all of them\n",
        "        n_slice  : work when type equals to 'region', height & width will equally sliced into N slices\n",
        "        normalize: normalize output histogram\n",
        "  \n",
        "      return\n",
        "        type == 'global'\n",
        "          a numpy array with size R\n",
        "        type == 'region'\n",
        "          a numpy array with size n_slice * n_slice * R\n",
        "  \n",
        "        #R = (rings * histograms + 1) * n_orient#\n",
        "    '''\n",
        "    if isinstance(input, np.ndarray):  # examinate input type\n",
        "      img = input.copy()\n",
        "    else:\n",
        "      img = scipy.misc.imread(input, mode='RGB')\n",
        "    height, width, channel = img.shape\n",
        "  \n",
        "    P = math.ceil((height - radius*2) / step) \n",
        "    Q = math.ceil((width - radius*2) / step)\n",
        "    assert P > 0 and Q > 0, \"input image size need to pass this check\"\n",
        "  \n",
        "    if type == 'global':\n",
        "      hist = self._daisy(img)\n",
        "  \n",
        "    elif type == 'region':\n",
        "      hist = np.zeros((n_slice, n_slice, R))\n",
        "      h_silce = np.around(np.linspace(0, height, n_slice+1, endpoint=True)).astype(int)\n",
        "      w_slice = np.around(np.linspace(0, width, n_slice+1, endpoint=True)).astype(int)\n",
        "  \n",
        "      for hs in range(len(h_silce)-1):\n",
        "        for ws in range(len(w_slice)-1):\n",
        "          img_r = img[h_silce[hs]:h_silce[hs+1], w_slice[ws]:w_slice[ws+1]]  # slice img to regions\n",
        "          hist[hs][ws] = self._daisy(img_r)\n",
        "  \n",
        "    if normalize:\n",
        "      hist /= np.sum(hist)\n",
        "  \n",
        "    return hist.flatten()\n",
        "  \n",
        "  \n",
        "  def _daisy(self, img, normalize=True):\n",
        "    image = color.rgb2gray(img)\n",
        "    descs = daisy(image, step=step, radius=radius, rings=rings, histograms=histograms, orientations=n_orient)\n",
        "    descs = descs.reshape(-1, R)  # shape=(N, R)\n",
        "    hist  = np.mean(descs, axis=0)  # shape=(R,)\n",
        "  \n",
        "    if normalize:\n",
        "      hist = np.array(hist) / np.sum(hist)\n",
        "  \n",
        "    return hist\n",
        "  \n",
        "  \n",
        "  def make_samples(self, db, verbose=True):\n",
        "    if h_type == 'global':\n",
        "      sample_cache = \"daisy-{}-n_orient{}-step{}-radius{}-rings{}-histograms{}\".format(h_type, n_orient, step, radius, rings, histograms)\n",
        "    elif h_type == 'region':\n",
        "      sample_cache = \"daisy-{}-n_slice{}-n_orient{}-step{}-radius{}-rings{}-histograms{}\".format(h_type, n_slice, n_orient, step, radius, rings, histograms)\n",
        "  \n",
        "    try:\n",
        "      samples = cPickle.load(open(os.path.join(cache_dir, sample_cache), \"rb\", True))\n",
        "      for sample in samples:\n",
        "        sample['hist'] /= np.sum(sample['hist'])  # normalize\n",
        "      if verbose:\n",
        "        print(\"Using cache..., config=%s, distance=%s, depth=%s\" % (sample_cache, d_type, depth))\n",
        "    except:\n",
        "      if verbose:\n",
        "        print(\"Counting histogram..., config=%s, distance=%s, depth=%s\" % (sample_cache, d_type, depth))\n",
        "  \n",
        "      samples = []\n",
        "      data = db.get_data()\n",
        "      for d in data.itertuples():\n",
        "        d_img, d_cls = getattr(d, \"img\"), getattr(d, \"cls\")\n",
        "        d_hist = self.histogram(d_img, type=h_type, n_slice=n_slice)\n",
        "        samples.append({\n",
        "                        'img':  d_img, \n",
        "                        'cls':  d_cls, \n",
        "                        'hist': d_hist\n",
        "                      })\n",
        "      cPickle.dump(samples, open(os.path.join(cache_dir, sample_cache), \"wb\", True))\n",
        "  \n",
        "    return samples\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  db = Database()\n",
        "\n",
        "  # evaluate database\n",
        "  APs = evaluate_class(db, f_class=Daisy, d_type=d_type, depth=depth)\n",
        "  cls_MAPs = []\n",
        "  for cls, cls_APs in APs.items():\n",
        "    MAP = np.mean(cls_APs)\n",
        "    print(\"Class {}, MAP {}\".format(cls, MAP))\n",
        "    cls_MAPs.append(MAP)\n",
        "  print(\"MMAP\", np.mean(cls_MAPs))\n"
      ],
      "metadata": {
        "id": "d66AVoD-HRay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "from evaluate import evaluate_class\n",
        "from DB import Database\n",
        "\n",
        "from six.moves import cPickle\n",
        "import numpy as np\n",
        "import scipy.misc\n",
        "from math import sqrt\n",
        "import os\n",
        "\n",
        "\n",
        "stride = (1, 1)\n",
        "n_slice  = 10\n",
        "h_type   = 'region'\n",
        "d_type   = 'cosine'\n",
        "\n",
        "depth    = 5\n",
        "\n",
        "''' MMAP    \n",
        "      depth\n",
        "       depthNone, region-stride(1, 1)-n_slice10,co, MMAP 0.101670982288\n",
        "       depth100,  region-stride(1, 1)-n_slice10,co, MMAP 0.207817305128\n",
        "       depth30,   region-stride(1, 1)-n_slice10,co, MMAP 0.291715090839\n",
        "       depth10,   region-stride(1, 1)-n_slice10,co, MMAP 0.353722379063\n",
        "       depth5,    region-stride(1, 1)-n_slice10,co, MMAP 0.367119444444\n",
        "       depth3,    region-stride(1, 1)-n_slice10,co, MMAP 0.3585\n",
        "       depth1,    region-stride(1, 1)-n_slice10,co, MMAP 0.302\n",
        "  \n",
        "       (exps below use depth=None)\n",
        "  \n",
        "      d_type\n",
        "       global-stride(2, 2),d1, MMAP 0.0530993236031\n",
        "       global-stride(2, 2),co, MMAP 0.0528310744618\n",
        "  \n",
        "      stride\n",
        "       region-stride(2, 2)-n_slice4,d1, MMAP 0.0736245142237\n",
        "       region-stride(1, 1)-n_slice4,d1, MMAP 0.0704206226545\n",
        "  \n",
        "      n_slice\n",
        "       region-stride(1, 1)-n_slice10,co, MMAP 0.101670982288\n",
        "       region-stride(1, 1)-n_slice6,co, MMAP 0.0977736743859\n",
        "  \n",
        "      h_type\n",
        "       global-stride(2, 2),d1, MMAP 0.0530993236031\n",
        "       region-stride(2, 2)-n_slice4,d1, MMAP 0.0736245142237\n",
        "'''\n",
        "\n",
        "edge_kernels = np.array([\n",
        "  [\n",
        "   # vertical\n",
        "   [1,-1], \n",
        "   [1,-1]\n",
        "  ],\n",
        "  [\n",
        "   # horizontal\n",
        "   [1,1], \n",
        "   [-1,-1]\n",
        "  ],\n",
        "  [\n",
        "   # 45 diagonal\n",
        "   [sqrt(2),0], \n",
        "   [0,-sqrt(2)]\n",
        "  ],\n",
        "  [\n",
        "   # 135 diagnol\n",
        "   [0,sqrt(2)], \n",
        "   [-sqrt(2),0]\n",
        "  ],\n",
        "  [\n",
        "   # non-directional\n",
        "   [2,-2], \n",
        "   [-2,2]\n",
        "  ]\n",
        "])\n",
        "\n",
        "# cache dir\n",
        "cache_dir = 'cache'\n",
        "if not os.path.exists(cache_dir):\n",
        "  os.makedirs(cache_dir)\n",
        "\n",
        "\n",
        "class Edge(object):\n",
        "\n",
        "  def histogram(self, input, stride=(2, 2), type=h_type, n_slice=n_slice, normalize=True):\n",
        "    ''' count img histogram\n",
        "  \n",
        "      arguments\n",
        "        input    : a path to a image or a numpy.ndarray\n",
        "        stride   : stride of edge kernel\n",
        "        type     : 'global' means count the histogram for whole image\n",
        "                   'region' means count the histogram for regions in images, then concatanate all of them\n",
        "        n_slice  : work when type equals to 'region', height & width will equally sliced into N slices\n",
        "        normalize: normalize output histogram\n",
        "  \n",
        "      return\n",
        "        type == 'global'\n",
        "          a numpy array with size len(edge_kernels)\n",
        "        type == 'region'\n",
        "          a numpy array with size len(edge_kernels) * n_slice * n_slice\n",
        "    '''\n",
        "    if isinstance(input, np.ndarray):  # examinate input type\n",
        "      img = input.copy()\n",
        "    else:\n",
        "      img = scipy.misc.imread(input, mode='RGB')\n",
        "    height, width, channel = img.shape\n",
        "  \n",
        "    if type == 'global':\n",
        "      hist = self._conv(img, stride=stride, kernels=edge_kernels)\n",
        "  \n",
        "    elif type == 'region':\n",
        "      hist = np.zeros((n_slice, n_slice, edge_kernels.shape[0]))\n",
        "      h_silce = np.around(np.linspace(0, height, n_slice+1, endpoint=True)).astype(int)\n",
        "      w_slice = np.around(np.linspace(0, width, n_slice+1, endpoint=True)).astype(int)\n",
        "  \n",
        "      for hs in range(len(h_silce)-1):\n",
        "        for ws in range(len(w_slice)-1):\n",
        "          img_r = img[h_silce[hs]:h_silce[hs+1], w_slice[ws]:w_slice[ws+1]]  # slice img to regions\n",
        "          hist[hs][ws] = self._conv(img_r, stride=stride, kernels=edge_kernels)\n",
        "  \n",
        "    if normalize:\n",
        "      hist /= np.sum(hist)\n",
        "  \n",
        "    return hist.flatten()\n",
        "  \n",
        "  \n",
        "  def _conv(self, img, stride, kernels, normalize=True):\n",
        "    H, W, C = img.shape\n",
        "    conv_kernels = np.expand_dims(kernels, axis=3)\n",
        "    conv_kernels = np.tile(conv_kernels, (1, 1, 1, C))\n",
        "    assert list(conv_kernels.shape) == list(kernels.shape) + [C]  # check kernels size\n",
        "  \n",
        "    sh, sw = stride\n",
        "    kn, kh, kw, kc = conv_kernels.shape\n",
        "  \n",
        "    hh = int((H - kh) / sh + 1)\n",
        "    ww = int((W - kw) / sw + 1)\n",
        "  \n",
        "    hist = np.zeros(kn)\n",
        "  \n",
        "    for idx, k in enumerate(conv_kernels):\n",
        "      for h in range(hh):\n",
        "        hs = int(h*sh)\n",
        "        he = int(h*sh + kh)\n",
        "        for w in range(ww):\n",
        "          ws = w*sw\n",
        "          we = w*sw + kw\n",
        "          hist[idx] += np.sum(img[hs:he, ws:we] * k)  # element-wise product\n",
        "  \n",
        "    if normalize:\n",
        "      hist /= np.sum(hist)\n",
        "  \n",
        "    return hist\n",
        "  \n",
        "  \n",
        "  def make_samples(self, db, verbose=True):\n",
        "    if h_type == 'global':\n",
        "      sample_cache = \"edge-{}-stride{}\".format(h_type, stride)\n",
        "    elif h_type == 'region':\n",
        "      sample_cache = \"edge-{}-stride{}-n_slice{}\".format(h_type, stride, n_slice)\n",
        "  \n",
        "    try:\n",
        "      samples = cPickle.load(open(os.path.join(cache_dir, sample_cache), \"rb\", True))\n",
        "      for sample in samples:\n",
        "        sample['hist'] /= np.sum(sample['hist'])  # normalize\n",
        "      if verbose:\n",
        "        print(\"Using cache..., config=%s, distance=%s, depth=%s\" % (sample_cache, d_type, depth))\n",
        "    except:\n",
        "      if verbose:\n",
        "        print(\"Counting histogram..., config=%s, distance=%s, depth=%s\" % (sample_cache, d_type, depth))\n",
        "  \n",
        "      samples = []\n",
        "      data = db.get_data()\n",
        "      for d in data.itertuples():\n",
        "        d_img, d_cls = getattr(d, \"img\"), getattr(d, \"cls\")\n",
        "        d_hist = self.histogram(d_img, type=h_type, n_slice=n_slice)\n",
        "        samples.append({\n",
        "                        'img':  d_img, \n",
        "                        'cls':  d_cls, \n",
        "                        'hist': d_hist\n",
        "                      })\n",
        "      cPickle.dump(samples, open(os.path.join(cache_dir, sample_cache), \"wb\", True))\n",
        "  \n",
        "    return samples\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  db = Database()\n",
        "\n",
        "  # check shape\n",
        "  assert edge_kernels.shape == (5, 2, 2)\n",
        "\n",
        "  # evaluate database\n",
        "  APs = evaluate_class(db, f_class=Edge, d_type=d_type, depth=depth)\n",
        "  cls_MAPs = []\n",
        "  for cls, cls_APs in APs.items():\n",
        "    MAP = np.mean(cls_APs)\n",
        "    print(\"Class {}, MAP {}\".format(cls, MAP))\n",
        "    cls_MAPs.append(MAP)\n",
        "  print(\"MMAP\", np.mean(cls_MAPs))\n"
      ],
      "metadata": {
        "id": "XOacF9qhHTte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "from scipy import spatial\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class Evaluation(object):\n",
        "\n",
        "  def make_samples(self):\n",
        "    raise NotImplementedError(\"Needs to implemented this method\")\n",
        "\n",
        "\n",
        "def distance(v1, v2, d_type='d1'):\n",
        "  assert v1.shape == v2.shape, \"shape of two vectors need to be same!\"\n",
        "\n",
        "  if d_type == 'd1':\n",
        "    return np.sum(np.absolute(v1 - v2))\n",
        "  elif d_type == 'd2':\n",
        "    return np.sum((v1 - v2) ** 2)\n",
        "  elif d_type == 'd2-norm':\n",
        "    return 2 - 2 * np.dot(v1, v2)\n",
        "  elif d_type == 'd3':\n",
        "    pass\n",
        "  elif d_type == 'd4':\n",
        "    pass\n",
        "  elif d_type == 'd5':\n",
        "    pass\n",
        "  elif d_type == 'd6':\n",
        "    pass\n",
        "  elif d_type == 'd7':\n",
        "    return 2 - 2 * np.dot(v1, v2)\n",
        "  elif d_type == 'd8':\n",
        "    return 2 - 2 * np.dot(v1, v2)\n",
        "  elif d_type == 'cosine':\n",
        "    return spatial.distance.cosine(v1, v2)\n",
        "  elif d_type == 'square':\n",
        "    return np.sum((v1 - v2) ** 2)\n",
        "\n",
        "\n",
        "def AP(label, results, sort=True):\n",
        "  ''' infer a query, return it's ap\n",
        "\n",
        "    arguments\n",
        "      label  : query's class\n",
        "      results: a dict with two keys, see the example below\n",
        "               {\n",
        "                 'dis': <distance between sample & query>,\n",
        "                 'cls': <sample's class>\n",
        "               }\n",
        "      sort   : sort the results by distance\n",
        "  '''\n",
        "  if sort:\n",
        "    results = sorted(results, key=lambda x: x['dis'])\n",
        "  precision = []\n",
        "  hit = 0\n",
        "  for i, result in enumerate(results):\n",
        "    if result['cls'] == label:\n",
        "      hit += 1\n",
        "      precision.append(hit / (i+1.))\n",
        "  if hit == 0:\n",
        "    return 0.\n",
        "  return np.mean(precision)\n",
        "\n",
        "\n",
        "def infer(query, samples=None, db=None, sample_db_fn=None, depth=None, d_type='d1'):\n",
        "  ''' infer a query, return it's ap\n",
        "\n",
        "    arguments\n",
        "      query       : a dict with three keys, see the template\n",
        "                    {\n",
        "                      'img': <path_to_img>,\n",
        "                      'cls': <img class>,\n",
        "                      'hist' <img histogram>\n",
        "                    }\n",
        "      samples     : a list of {\n",
        "                                'img': <path_to_img>,\n",
        "                                'cls': <img class>,\n",
        "                                'hist' <img histogram>\n",
        "                              }\n",
        "      db          : an instance of class Database\n",
        "      sample_db_fn: a function making samples, should be given if Database != None\n",
        "      depth       : retrieved depth during inference, the default depth is equal to database size\n",
        "      d_type      : distance type\n",
        "  '''\n",
        "  assert samples != None or (db != None and sample_db_fn != None), \"need to give either samples or db plus sample_db_fn\"\n",
        "  if db:\n",
        "    samples = sample_db_fn(db)\n",
        "\n",
        "  q_img, q_cls, q_hist = query['img'], query['cls'], query['hist']\n",
        "  results = []\n",
        "  for idx, sample in enumerate(samples):\n",
        "    s_img, s_cls, s_hist = sample['img'], sample['cls'], sample['hist']\n",
        "    if q_img == s_img:\n",
        "      continue\n",
        "    results.append({\n",
        "                    'dis': distance(q_hist, s_hist, d_type=d_type),\n",
        "                    'cls': s_cls\n",
        "                  })\n",
        "  results = sorted(results, key=lambda x: x['dis'])\n",
        "  if depth and depth <= len(results):\n",
        "    results = results[:depth]\n",
        "  ap = AP(q_cls, results, sort=False)\n",
        "\n",
        "  return ap, results\n",
        "\n",
        "\n",
        "def evaluate(db, sample_db_fn, depth=None, d_type='d1'):\n",
        "  ''' infer the whole database\n",
        "\n",
        "    arguments\n",
        "      db          : an instance of class Database\n",
        "      sample_db_fn: a function making samples, should be given if Database != None\n",
        "      depth       : retrieved depth during inference, the default depth is equal to database size\n",
        "      d_type      : distance type\n",
        "  '''\n",
        "  classes = db.get_class()\n",
        "  ret = {c: [] for c in classes}\n",
        "\n",
        "  samples = sample_db_fn(db)\n",
        "  for query in samples:\n",
        "    ap, _ = infer(query, samples=samples, depth=depth, d_type=d_type)\n",
        "    ret[query['cls']].append(ap)\n",
        "\n",
        "  return ret\n",
        "\n",
        "\n",
        "def evaluate_class(db, f_class=None, f_instance=None, depth=None, d_type='d1'):\n",
        "  ''' infer the whole database\n",
        "\n",
        "    arguments\n",
        "      db     : an instance of class Database\n",
        "      f_class: a class that generate features, needs to implement make_samples method\n",
        "      depth  : retrieved depth during inference, the default depth is equal to database size\n",
        "      d_type : distance type\n",
        "  '''\n",
        "  assert f_class or f_instance, \"needs to give class_name or an instance of class\"\n",
        "\n",
        "  classes = db.get_class()\n",
        "  ret = {c: [] for c in classes}\n",
        "\n",
        "  if f_class:\n",
        "    f = f_class()\n",
        "  elif f_instance:\n",
        "    f = f_instance\n",
        "  samples = f.make_samples(db)\n",
        "  for query in samples:\n",
        "    ap, _ = infer(query, samples=samples, depth=depth, d_type=d_type)\n",
        "    ret[query['cls']].append(ap)\n",
        "\n",
        "  return ret\n"
      ],
      "metadata": {
        "id": "2eHcT5_8HV8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "from evaluate import evaluate_class\n",
        "from DB import Database\n",
        "\n",
        "from color import Color\n",
        "from daisy import Daisy\n",
        "from edge  import Edge\n",
        "from gabor import Gabor\n",
        "from HOG   import HOG\n",
        "from vggnet import VGGNetFeat\n",
        "from resnet import ResNetFeat\n",
        "\n",
        "import numpy as np\n",
        "import itertools\n",
        "import os\n",
        "\n",
        "\n",
        "d_type   = 'd1'\n",
        "depth    = 30\n",
        "\n",
        "feat_pools = ['color', 'daisy', 'edge', 'gabor', 'hog', 'vgg', 'res']\n",
        "\n",
        "# result dir\n",
        "result_dir = 'result'\n",
        "if not os.path.exists(result_dir):\n",
        "  os.makedirs(result_dir)\n",
        "\n",
        "\n",
        "class FeatureFusion(object):\n",
        "\n",
        "  def __init__(self, features):\n",
        "    assert len(features) > 1, \"need to fuse more than one feature!\"\n",
        "    self.features = features\n",
        "    self.samples  = None\n",
        "\n",
        "  def make_samples(self, db, verbose=False):\n",
        "    if verbose:\n",
        "      print(\"Use features {}\".format(\" & \".join(self.features)))\n",
        "\n",
        "    if self.samples == None:\n",
        "      feats = []\n",
        "      for f_class in self.features:\n",
        "        feats.append(self._get_feat(db, f_class))\n",
        "      samples = self._concat_feat(db, feats)\n",
        "      self.samples = samples  # cache the result\n",
        "    return self.samples\n",
        "\n",
        "  def _get_feat(self, db, f_class):\n",
        "    if f_class == 'color':\n",
        "      f_c = Color()\n",
        "    elif f_class == 'daisy':\n",
        "      f_c = Daisy()\n",
        "    elif f_class == 'edge':\n",
        "      f_c = Edge()\n",
        "    elif f_class == 'gabor':\n",
        "      f_c = Gabor()\n",
        "    elif f_class == 'hog':\n",
        "      f_c = HOG()\n",
        "    elif f_class == 'vgg':\n",
        "      f_c = VGGNetFeat()\n",
        "    elif f_class == 'res':\n",
        "      f_c = ResNetFeat()\n",
        "    return f_c.make_samples(db, verbose=False)\n",
        "\n",
        "  def _concat_feat(self, db, feats):\n",
        "    samples = feats[0]\n",
        "    delete_idx = []\n",
        "    for idx in range(len(samples)):\n",
        "      for feat in feats[1:]:\n",
        "        feat = self._to_dict(feat)\n",
        "        key = samples[idx]['img']\n",
        "        if key not in feat:\n",
        "          delete_idx.append(idx)\n",
        "          continue\n",
        "        assert feat[key]['cls'] == samples[idx]['cls']\n",
        "        samples[idx]['hist'] = np.append(samples[idx]['hist'], feat[key]['hist'])\n",
        "    for d_idx in sorted(set(delete_idx), reverse=True):\n",
        "      del samples[d_idx]\n",
        "    if delete_idx != []:\n",
        "      print(\"Ignore %d samples\" % len(set(delete_idx)))\n",
        "\n",
        "    return samples\n",
        "\n",
        "  def _to_dict(self, feat):\n",
        "    ret = {}\n",
        "    for f in feat:\n",
        "      ret[f['img']] = {\n",
        "        'cls': f['cls'],\n",
        "        'hist': f['hist']\n",
        "      }\n",
        "    return ret\n",
        "\n",
        "\n",
        "def evaluate_feats(db, N, feat_pools=feat_pools, d_type='d1', depths=[None, 300, 200, 100, 50, 30, 10, 5, 3, 1]):\n",
        "  result = open(os.path.join(result_dir, 'feature_fusion-{}-{}feats.csv'.format(d_type, N)), 'w')\n",
        "  for i in range(N):\n",
        "    result.write(\"feat{},\".format(i))\n",
        "  result.write(\"depth,distance,MMAP\")\n",
        "  combinations = itertools.combinations(feat_pools, N)\n",
        "  for combination in combinations:\n",
        "    fusion = FeatureFusion(features=list(combination))\n",
        "    for d in depths:\n",
        "      APs = evaluate_class(db, f_instance=fusion, d_type=d_type, depth=d)\n",
        "      cls_MAPs = []\n",
        "      for cls, cls_APs in APs.items():\n",
        "        MAP = np.mean(cls_APs)\n",
        "        cls_MAPs.append(MAP)\n",
        "      r = \"{},{},{},{}\".format(\",\".join(combination), d, d_type, np.mean(cls_MAPs))\n",
        "      print(r)\n",
        "      result.write('\\n'+r)\n",
        "    print()\n",
        "  result.close()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  db = Database()\n",
        "\n",
        "  # evaluate features double-wise\n",
        "  evaluate_feats(db, N=2, d_type='d1')\n",
        "\n",
        "  # evaluate features triple-wise\n",
        "  evaluate_feats(db, N=3, d_type='d1')\n",
        "  \n",
        "  # evaluate features quadra-wise\n",
        "  evaluate_feats(db, N=4, d_type='d1')\n",
        "\n",
        "  # evaluate features penta-wise\n",
        "  evaluate_feats(db, N=5, d_type='d1')\n",
        "\n",
        "  # evaluate features hexa-wise\n",
        "  evaluate_feats(db, N=6, d_type='d1')\n",
        "\n",
        "  # evaluate features hepta-wise\n",
        "  evaluate_feats(db, N=7, d_type='d1')\n",
        "  \n",
        "  # evaluate database\n",
        "  fusion = FeatureFusion(features=['color', 'daisy'])\n",
        "  APs = evaluate_class(db, f_instance=fusion, d_type=d_type, depth=depth)\n",
        "  cls_MAPs = []\n",
        "  for cls, cls_APs in APs.items():\n",
        "    MAP = np.mean(cls_APs)\n",
        "    print(\"Class {}, MAP {}\".format(cls, MAP))\n",
        "    cls_MAPs.append(MAP)\n",
        "  print(\"MMAP\", np.mean(cls_MAPs))\n"
      ],
      "metadata": {
        "id": "Ea0KI53mHYtX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "from evaluate import *\n",
        "from DB import Database\n",
        "\n",
        "from skimage.filters import gabor_kernel\n",
        "from skimage import color\n",
        "from scipy import ndimage as ndi\n",
        "\n",
        "import multiprocessing\n",
        "\n",
        "from six.moves import cPickle\n",
        "import numpy as np\n",
        "import scipy.misc\n",
        "import os\n",
        "\n",
        "\n",
        "theta     = 4\n",
        "frequency = (0.1, 0.5, 0.8)\n",
        "sigma     = (1, 3, 5)\n",
        "bandwidth = (0.3, 0.7, 1)\n",
        "\n",
        "n_slice  = 2\n",
        "h_type   = 'global'\n",
        "d_type   = 'cosine'\n",
        "\n",
        "depth    = 1\n",
        "\n",
        "''' MMAP\n",
        "     depth\n",
        "      depthNone, global-theta4-frequency(0.1, 0.5, 0.8)-sigma(1, 3, 5)-bandwidth(0.3, 0.7, 1), distance=cosine, MMAP 0.141136758233\n",
        "      depth100,  global-theta4-frequency(0.1, 0.5, 0.8)-sigma(1, 3, 5)-bandwidth(0.3, 0.7, 1), distance=cosine, MMAP 0.216985780572\n",
        "      depth30,   global-theta4-frequency(0.1, 0.5, 0.8)-sigma(1, 3, 5)-bandwidth(0.3, 0.7, 1), distance=cosine, MMAP 0.310063286599\n",
        "      depth10,   global-theta4-frequency(0.1, 0.5, 0.8)-sigma(1, 3, 5)-bandwidth(0.3, 0.7, 1), distance=cosine, MMAP 0.3847025\n",
        "      depth5,    global-theta4-frequency(0.1, 0.5, 0.8)-sigma(1, 3, 5)-bandwidth(0.3, 0.7, 1), distance=cosine, MMAP 0.400002777778\n",
        "      depth3,    global-theta4-frequency(0.1, 0.5, 0.8)-sigma(1, 3, 5)-bandwidth(0.3, 0.7, 1), distance=cosine, MMAP 0.398166666667\n",
        "      depth1,    global-theta4-frequency(0.1, 0.5, 0.8)-sigma(1, 3, 5)-bandwidth(0.3, 0.7, 1), distance=cosine, MMAP 0.334\n",
        "\n",
        "     (exps below use depth=None)\n",
        "\n",
        "     _power\n",
        "      gabor-global-theta4-frequency(0.1, 0.5, 0.8)-sigma(0.05, 0.25)-bandwidthNone, distance=cosine, MMAP 0.0821975313939\n",
        "      gabor-global-theta6-frequency(0.1, 0.5)-sigma(1, 3)-bandwidth(0.5, 1), distance=cosine, MMAP 0.139570979988\n",
        "      gabor-global-theta6-frequency(0.1, 0.8)-sigma(1, 3)-bandwidth(0.7, 1), distance=cosine, MMAP 0.139554792177\n",
        "      gabor-global-theta8-frequency(0.1, 0.5, 0.8)-sigma(1, 3, 5)-bandwidth(0.3, 0.7, 1), distance=cosine, MMAP 0.140947344315\n",
        "      gabor-global-theta6-frequency(0.1, 0.5, 0.8)-sigma(1, 3, 5)-bandwidth(0.3, 0.7, 1), distance=cosine, MMAP 0.139914401079\n",
        "      gabor-global-theta4-frequency(0.1, 0.5, 0.8)-sigma(1, 3, 5)-bandwidth(0.3, 0.7, 1), distance=cosine, MMAP 0.141136758233\n",
        "      gabor-global-theta4-frequency(0.1, 0.5, 1)-sigma(0.25, 1)-bandwidth(0.5, 1), distance=cosine, MMAP 0.120351804156\n",
        "'''\n",
        "\n",
        "def make_gabor_kernel(theta, frequency, sigma, bandwidth):\n",
        "  kernels = []\n",
        "  for t in range(theta):\n",
        "    t = t / float(theta) * np.pi\n",
        "    for f in frequency:\n",
        "      if sigma:\n",
        "        for s in sigma:\n",
        "          kernel = gabor_kernel(f, theta=t, sigma_x=s, sigma_y=s)\n",
        "          kernels.append(kernel)\n",
        "      if bandwidth:\n",
        "        for b in bandwidth:\n",
        "          kernel = gabor_kernel(f, theta=t, bandwidth=b)\n",
        "          kernels.append(kernel)\n",
        "  return kernels\n",
        "\n",
        "gabor_kernels = make_gabor_kernel(theta, frequency, sigma, bandwidth)\n",
        "if sigma and not bandwidth:\n",
        "  assert len(gabor_kernels) == theta * len(frequency) * len(sigma), \"kernel nums error in make_gabor_kernel()\"\n",
        "elif not sigma and bandwidth:\n",
        "  assert len(gabor_kernels) == theta * len(frequency) * len(bandwidth), \"kernel nums error in make_gabor_kernel()\"\n",
        "elif sigma and bandwidth:\n",
        "  assert len(gabor_kernels) == theta * len(frequency) * (len(sigma) + len(bandwidth)), \"kernel nums error in make_gabor_kernel()\"\n",
        "elif not sigma and not bandwidth:\n",
        "  assert len(gabor_kernels) == theta * len(frequency), \"kernel nums error in make_gabor_kernel()\"\n",
        "\n",
        "# cache dir\n",
        "cache_dir = 'cache'\n",
        "if not os.path.exists(cache_dir):\n",
        "  os.makedirs(cache_dir)\n",
        "\n",
        "\n",
        "class Gabor(object):  \n",
        "  \n",
        "  def gabor_histogram(self, input, type=h_type, n_slice=n_slice, normalize=True):\n",
        "    ''' count img histogram\n",
        "  \n",
        "      arguments\n",
        "        input    : a path to a image or a numpy.ndarray\n",
        "        type     : 'global' means count the histogram for whole image\n",
        "                   'region' means count the histogram for regions in images, then concatanate all of them\n",
        "        n_slice  : work when type equals to 'region', height & width will equally sliced into N slices\n",
        "        normalize: normalize output histogram\n",
        "  \n",
        "      return\n",
        "        type == 'global'\n",
        "          a numpy array with size len(gabor_kernels)\n",
        "        type == 'region'\n",
        "          a numpy array with size len(gabor_kernels) * n_slice * n_slice\n",
        "    '''\n",
        "    if isinstance(input, np.ndarray):  # examinate input type\n",
        "      img = input.copy()\n",
        "    else:\n",
        "      img = scipy.misc.imread(input, mode='RGB')\n",
        "    height, width, channel = img.shape\n",
        "  \n",
        "    if type == 'global':\n",
        "      hist = self._gabor(img, kernels=gabor_kernels)\n",
        "  \n",
        "    elif type == 'region':\n",
        "      hist = np.zeros((n_slice, n_slice, len(gabor_kernels)))\n",
        "      h_silce = np.around(np.linspace(0, height, n_slice+1, endpoint=True)).astype(int)\n",
        "      w_slice = np.around(np.linspace(0, width, n_slice+1, endpoint=True)).astype(int)\n",
        "  \n",
        "      for hs in range(len(h_silce)-1):\n",
        "        for ws in range(len(w_slice)-1):\n",
        "          img_r = img[h_silce[hs]:h_silce[hs+1], w_slice[ws]:w_slice[ws+1]]  # slice img to regions\n",
        "          hist[hs][ws] = self._gabor(img_r, kernels=gabor_kernels)\n",
        "  \n",
        "    if normalize:\n",
        "      hist /= np.sum(hist)\n",
        "  \n",
        "    return hist.flatten()\n",
        "  \n",
        "  \n",
        "  def _feats(self, image, kernel):\n",
        "    '''\n",
        "      arguments\n",
        "        image : ndarray of the image\n",
        "        kernel: a gabor kernel\n",
        "      return\n",
        "        a ndarray whose shape is (2, )\n",
        "    '''\n",
        "    feats = np.zeros(2, dtype=np.double)\n",
        "    filtered = ndi.convolve(image, np.real(kernel), mode='wrap')\n",
        "    feats[0] = filtered.mean()\n",
        "    feats[1] = filtered.var()\n",
        "    return feats\n",
        "  \n",
        "  \n",
        "  def _power(self, image, kernel):\n",
        "    '''\n",
        "      arguments\n",
        "        image : ndarray of the image\n",
        "        kernel: a gabor kernel\n",
        "      return\n",
        "        a ndarray whose shape is (2, )\n",
        "    '''\n",
        "    image = (image - image.mean()) / image.std()  # Normalize images for better comparison.\n",
        "    f_img = np.sqrt(ndi.convolve(image, np.real(kernel), mode='wrap')**2 +\n",
        "                   ndi.convolve(image, np.imag(kernel), mode='wrap')**2)\n",
        "    feats = np.zeros(2, dtype=np.double)\n",
        "    feats[0] = f_img.mean()\n",
        "    feats[1] = f_img.var()\n",
        "    return feats\n",
        "  \n",
        "  \n",
        "  def _gabor(self, image, kernels=make_gabor_kernel(theta, frequency, sigma, bandwidth), normalize=True):\n",
        "    pool = multiprocessing.Pool(processes=multiprocessing.cpu_count())\n",
        "  \n",
        "    img = color.rgb2gray(image)\n",
        "  \n",
        "    results = []\n",
        "    feat_fn = self._power\n",
        "    for kernel in kernels:\n",
        "      results.append(pool.apply_async(self._worker, (img, kernel, feat_fn)))\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "    \n",
        "    hist = np.array([res.get() for res in results])\n",
        "  \n",
        "    if normalize:\n",
        "      hist = hist / np.sum(hist, axis=0)\n",
        "  \n",
        "    return hist.T.flatten()\n",
        "  \n",
        "  \n",
        "  def _worker(self, img, kernel, feat_fn):\n",
        "    try:\n",
        "      ret = feat_fn(img, kernel)\n",
        "    except:\n",
        "      print(\"return zero\")\n",
        "      ret = np.zeros(2)\n",
        "    return ret\n",
        "  \n",
        "  \n",
        "  def make_samples(self, db, verbose=True):\n",
        "    if h_type == 'global':\n",
        "      sample_cache = \"gabor-{}-theta{}-frequency{}-sigma{}-bandwidth{}\".format(h_type, theta, frequency, sigma, bandwidth)\n",
        "    elif h_type == 'region':\n",
        "      sample_cache = \"gabor-{}-n_slice{}-theta{}-frequency{}-sigma{}-bandwidth{}\".format(h_type, n_slice, theta, frequency, sigma, bandwidth)\n",
        "  \n",
        "    try:\n",
        "      samples = cPickle.load(open(os.path.join(cache_dir, sample_cache), \"rb\", True))\n",
        "      for sample in samples:\n",
        "        sample['hist'] /= np.sum(sample['hist'])  # normalize\n",
        "      if verbose:\n",
        "        print(\"Using cache..., config=%s, distance=%s, depth=%s\" % (sample_cache, d_type, depth))\n",
        "    except:\n",
        "      if verbose:\n",
        "        print(\"Counting histogram..., config=%s, distance=%s, depth=%s\" % (sample_cache, d_type, depth))\n",
        "  \n",
        "      samples = []\n",
        "      data = db.get_data()\n",
        "      for d in data.itertuples():\n",
        "        d_img, d_cls = getattr(d, \"img\"), getattr(d, \"cls\")\n",
        "        d_hist = self.gabor_histogram(d_img, type=h_type, n_slice=n_slice)\n",
        "        samples.append({\n",
        "                        'img':  d_img, \n",
        "                        'cls':  d_cls, \n",
        "                        'hist': d_hist\n",
        "                      })\n",
        "      cPickle.dump(samples, open(os.path.join(cache_dir, sample_cache), \"wb\", True))\n",
        "  \n",
        "    return samples\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  db = Database()\n",
        "\n",
        "  # evaluate database\n",
        "  APs = evaluate_class(db, f_class=Gabor, d_type=d_type, depth=depth)\n",
        "  cls_MAPs = []\n",
        "  for cls, cls_APs in APs.items():\n",
        "    MAP = np.mean(cls_APs)\n",
        "    print(\"Class {}, MAP {}\".format(cls, MAP))\n",
        "    cls_MAPs.append(MAP)\n",
        "  print(\"MMAP\", np.mean(cls_MAPs))\n"
      ],
      "metadata": {
        "id": "HcRl0p9ZHbKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "from evaluate import infer\n",
        "from DB import Database\n",
        "\n",
        "from color import Color\n",
        "from daisy import Daisy\n",
        "from edge  import Edge\n",
        "from gabor import Gabor\n",
        "from HOG   import HOG\n",
        "from vggnet import VGGNetFeat\n",
        "from resnet import ResNetFeat\n",
        "\n",
        "depth = 5\n",
        "d_type = 'd1'\n",
        "query_idx = 0\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  db = Database()\n",
        "\n",
        "  # retrieve by color\n",
        "  method = Color()\n",
        "  samples = method.make_samples(db)\n",
        "  query = samples[query_idx]\n",
        "  _, result = infer(query, samples=samples, depth=depth, d_type=d_type)\n",
        "  print(result)\n",
        "\n",
        "  # retrieve by daisy\n",
        "  method = Daisy()\n",
        "  samples = method.make_samples(db)\n",
        "  query = samples[query_idx]\n",
        "  _, result = infer(query, samples=samples, depth=depth, d_type=d_type)\n",
        "  print(result)\n",
        "\n",
        "  # retrieve by edge\n",
        "  method = Edge()\n",
        "  samples = method.make_samples(db)\n",
        "  query = samples[query_idx]\n",
        "  _, result = infer(query, samples=samples, depth=depth, d_type=d_type)\n",
        "  print(result)\n",
        "\n",
        "  # retrieve by gabor\n",
        "  method = Gabor()\n",
        "  samples = method.make_samples(db)\n",
        "  query = samples[query_idx]\n",
        "  _, result = infer(query, samples=samples, depth=depth, d_type=d_type)\n",
        "  print(result)\n",
        "\n",
        "  # retrieve by HOG\n",
        "  method = HOG()\n",
        "  samples = method.make_samples(db)\n",
        "  query = samples[query_idx]\n",
        "  _, result = infer(query, samples=samples, depth=depth, d_type=d_type)\n",
        "  print(result)\n",
        "\n",
        "  # retrieve by VGG\n",
        "  method = VGGNetFeat()\n",
        "  samples = method.make_samples(db)\n",
        "  query = samples[query_idx]\n",
        "  _, result = infer(query, samples=samples, depth=depth, d_type=d_type)\n",
        "  print(result)\n",
        "\n",
        "  # retrieve by resnet\n",
        "  method = ResNetFeat()\n",
        "  samples = method.make_samples(db)\n",
        "  query = samples[query_idx]\n",
        "  _, result = infer(query, samples=samples, depth=depth, d_type=d_type)\n",
        "  print(result)\n"
      ],
      "metadata": {
        "id": "uR5tUYM-HeAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "from evaluate import evaluate_class\n",
        "from DB import Database\n",
        "\n",
        "from color import Color\n",
        "from daisy import Daisy\n",
        "from edge  import Edge\n",
        "from gabor import Gabor\n",
        "from HOG   import HOG\n",
        "from vggnet import VGGNetFeat\n",
        "from resnet import ResNetFeat\n",
        "\n",
        "from sklearn.random_projection import johnson_lindenstrauss_min_dim\n",
        "from sklearn import random_projection\n",
        "import numpy as np\n",
        "import itertools\n",
        "import os\n",
        "\n",
        "\n",
        "feat_pools = ['color', 'daisy', 'edge', 'gabor', 'hog', 'vgg', 'res']\n",
        "\n",
        "keep_rate = 0.25\n",
        "project_type = 'sparse'\n",
        "\n",
        "# result dir\n",
        "result_dir = 'result'\n",
        "if not os.path.exists(result_dir):\n",
        "  os.makedirs(result_dir)\n",
        "\n",
        "\n",
        "class RandomProjection(object):\n",
        "\n",
        "  def __init__(self, features, keep_rate=keep_rate, project_type=project_type):\n",
        "    assert len(features) > 0, \"need to give at least one feature!\"\n",
        "    self.features     = features\n",
        "    self.keep_rate    = keep_rate\n",
        "    self.project_type = project_type\n",
        "\n",
        "    self.samples      = None\n",
        "\n",
        "  def make_samples(self, db, verbose=False):\n",
        "    if verbose:\n",
        "      print(\"Use features {}, {} RandomProject, keep {}\".format(\" & \".join(self.features), self.project_type, self.keep_rate))\n",
        "\n",
        "    if self.samples == None:\n",
        "      feats = []\n",
        "      for f_class in self.features:\n",
        "        feats.append(self._get_feat(db, f_class))\n",
        "      samples = self._concat_feat(db, feats)\n",
        "      samples, _ = self._rp(samples)\n",
        "      self.samples = samples  # cache the result\n",
        "    return self.samples\n",
        "\n",
        "  def check_random_projection(self):\n",
        "    ''' check if current smaple can fit to random project\n",
        "\n",
        "       return\n",
        "         a boolean\n",
        "    '''\n",
        "    if self.samples == None:\n",
        "      feats = []\n",
        "      for f_class in self.features:\n",
        "        feats.append(self._get_feat(db, f_class))\n",
        "      samples = self._concat_feat(db, feats)\n",
        "      samples, flag = self._rp(samples)\n",
        "      self.samples = samples  # cache the result\n",
        "    return True if flag else False\n",
        "\n",
        "  def _get_feat(self, db, f_class):\n",
        "    if f_class == 'color':\n",
        "      f_c = Color()\n",
        "    elif f_class == 'daisy':\n",
        "      f_c = Daisy()\n",
        "    elif f_class == 'edge':\n",
        "      f_c = Edge()\n",
        "    elif f_class == 'gabor':\n",
        "      f_c = Gabor()\n",
        "    elif f_class == 'hog':\n",
        "      f_c = HOG()\n",
        "    elif f_class == 'vgg':\n",
        "      f_c = VGGNetFeat()\n",
        "    elif f_class == 'res':\n",
        "      f_c = ResNetFeat()\n",
        "    return f_c.make_samples(db, verbose=False)\n",
        "\n",
        "  def _concat_feat(self, db, feats):\n",
        "    samples = feats[0]\n",
        "    delete_idx = []\n",
        "    for idx in range(len(samples)):\n",
        "      for feat in feats[1:]:\n",
        "        feat = self._to_dict(feat)\n",
        "        key = samples[idx]['img']\n",
        "        if key not in feat:\n",
        "          delete_idx.append(idx)\n",
        "          continue\n",
        "        assert feat[key]['cls'] == samples[idx]['cls']\n",
        "        samples[idx]['hist'] = np.append(samples[idx]['hist'], feat[key]['hist'])\n",
        "    for d_idx in sorted(set(delete_idx), reverse=True):\n",
        "      del samples[d_idx]\n",
        "    if delete_idx != []:\n",
        "      print(\"Ignore %d samples\" % len(set(delete_idx)))\n",
        "\n",
        "    return samples\n",
        "\n",
        "  def _to_dict(self, feat):\n",
        "    ret = {}\n",
        "    for f in feat:\n",
        "      ret[f['img']] = {\n",
        "        'cls': f['cls'],\n",
        "        'hist': f['hist']\n",
        "      }\n",
        "    return ret\n",
        "\n",
        "  def _rp(self, samples):\n",
        "    feats = np.array([s['hist'] for s in samples])\n",
        "    eps = self._get_eps(n_samples=feats.shape[0], n_dims=feats.shape[1])\n",
        "    if eps == -1:\n",
        "      import warnings\n",
        "      warnings.warn(\n",
        "        \"Can't fit to random projection with keep_rate {}\\n\".format(self.keep_rate), RuntimeWarning\n",
        "      )\n",
        "      return samples, False\n",
        "    if self.project_type == 'gaussian':\n",
        "      transformer = random_projection.GaussianRandomProjection(eps=eps) \n",
        "    elif self.project_type == 'sparse':\n",
        "      transformer = random_projection.SparseRandomProjection(eps=eps)\n",
        "    feats = transformer.fit_transform(feats)\n",
        "    assert feats.shape[0] == len(samples)\n",
        "    for idx in range(len(samples)):\n",
        "      samples[idx]['hist'] = feats[idx]\n",
        "    return samples, True\n",
        "\n",
        "  def _get_eps(self, n_samples, n_dims, n_slice=int(1e4)):\n",
        "    new_dim = n_dims * self.keep_rate\n",
        "    for i in range(1, n_slice):\n",
        "      eps = i / n_slice\n",
        "      jl_dim = johnson_lindenstrauss_min_dim(n_samples=n_samples, eps=eps)\n",
        "      if jl_dim <= new_dim:\n",
        "        print(\"rate %.3f, n_dims %d, new_dim %d, dims error rate: %.4f\" % (self.keep_rate, n_dims, jl_dim, ((new_dim-jl_dim) / new_dim)) )\n",
        "        return eps\n",
        "    return -1\n",
        "\n",
        "\n",
        "def evaluate_feats(db, N, feat_pools=feat_pools, keep_rate=keep_rate, project_type=project_type, d_type='d1', depths=[None, 300, 200, 100, 50, 30, 10, 5, 3, 1]):\n",
        "  result = open(os.path.join(result_dir, 'feature_reduction-{}-keep{}-{}-{}feats.csv'.format(project_type, keep_rate, d_type, N)), 'w')\n",
        "  for i in range(N):\n",
        "    result.write(\"feat{},\".format(i))\n",
        "  result.write(\"depth,distance,MMAP\")\n",
        "  combinations = itertools.combinations(feat_pools, N)\n",
        "  for combination in combinations:\n",
        "    fusion = RandomProjection(features=list(combination), keep_rate=keep_rate, project_type=project_type)\n",
        "    if fusion.check_random_projection():\n",
        "      for d in depths:\n",
        "        APs = evaluate_class(db, f_instance=fusion, d_type=d_type, depth=d)\n",
        "        cls_MAPs = []\n",
        "        for cls, cls_APs in APs.items():\n",
        "          MAP = np.mean(cls_APs)\n",
        "          cls_MAPs.append(MAP)\n",
        "        r = \"{},{},{},{}\".format(\",\".join(combination), d, d_type, np.mean(cls_MAPs))\n",
        "        print(r)\n",
        "        result.write('\\n'+r)\n",
        "      print()\n",
        "  result.close()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  db = Database()\n",
        "\n",
        "  # evaluate features single-wise\n",
        "  evaluate_feats(db, N=1, d_type='d1', keep_rate=keep_rate, project_type=project_type)\n",
        "\n",
        "  # evaluate features double-wise\n",
        "  evaluate_feats(db, N=2, d_type='d1', keep_rate=keep_rate, project_type=project_type)\n",
        "\n",
        "  # evaluate features triple-wise\n",
        "  evaluate_feats(db, N=3, d_type='d1', keep_rate=keep_rate, project_type=project_type)\n",
        "  \n",
        "  # evaluate features quadra-wise\n",
        "  evaluate_feats(db, N=4, d_type='d1', keep_rate=keep_rate, project_type=project_type)\n",
        "\n",
        "  # evaluate features penta-wise\n",
        "  evaluate_feats(db, N=5, d_type='d1', keep_rate=keep_rate, project_type=project_type)\n",
        "\n",
        "  # evaluate features hexa-wise\n",
        "  evaluate_feats(db, N=6, d_type='d1', keep_rate=keep_rate, project_type=project_type)\n",
        "\n",
        "  # evaluate features hepta-wise\n",
        "  evaluate_feats(db, N=7, d_type='d1', keep_rate=keep_rate, project_type=project_type)\n",
        "  \n",
        "  # evaluate color feature\n",
        "  d_type = 'd1'\n",
        "  depth  = 30\n",
        "  fusion = RandomProjection(features=['color'], keep_rate=keep_rate, project_type=project_type)\n",
        "  APs = evaluate_class(db, f_instance=fusion, d_type=d_type, depth=depth)\n",
        "  cls_MAPs = []\n",
        "  for cls, cls_APs in APs.items():\n",
        "    MAP = np.mean(cls_APs)\n",
        "    print(\"Class {}, MAP {}\".format(cls, MAP))\n",
        "    cls_MAPs.append(MAP)\n",
        "  print(\"MMAP\", np.mean(cls_MAPs))\n"
      ],
      "metadata": {
        "id": "9d5wT243HgIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torchvision import models\n",
        "from torchvision.models.resnet import Bottleneck, BasicBlock, ResNet\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "from six.moves import cPickle\n",
        "import numpy as np\n",
        "import scipy.misc\n",
        "import os\n",
        "\n",
        "from evaluate import evaluate_class\n",
        "from DB import Database\n",
        "\n",
        "\n",
        "'''\n",
        "  downloading problem in mac OSX should refer to this answer:\n",
        "    https://stackoverflow.com/a/42334357\n",
        "'''\n",
        "\n",
        "# configs for histogram\n",
        "RES_model  = 'resnet152'  # model type\n",
        "pick_layer = 'avg'        # extract feature of this layer\n",
        "d_type     = 'd1'         # distance type\n",
        "\n",
        "depth = 3  # retrieved depth, set to None will count the ap for whole database\n",
        "\n",
        "''' MMAP\n",
        "     depth\n",
        "      depthNone, resnet152,avg,d1, MMAP 0.78474710149\n",
        "      depth100,  resnet152,avg,d1, MMAP 0.819713653589\n",
        "      depth30,   resnet152,avg,d1, MMAP 0.884925001919\n",
        "      depth10,   resnet152,avg,d1, MMAP 0.944355078125\n",
        "      depth5,    resnet152,avg,d1, MMAP 0.961788675194\n",
        "      depth3,    resnet152,avg,d1, MMAP 0.965623938039\n",
        "      depth1,    resnet152,avg,d1, MMAP 0.958696281702\n",
        "\n",
        "      (exps below use depth=None)\n",
        "\n",
        "      resnet34,avg,cosine, MMAP 0.755842698037\n",
        "      resnet101,avg,cosine, MMAP 0.757435452078\n",
        "      resnet101,avg,d1, MMAP 0.764556148137\n",
        "      resnet152,avg,cosine, MMAP 0.776918319273\n",
        "      resnet152,avg,d1, MMAP 0.78474710149\n",
        "      resnet152,max,d1, MMAP 0.748099342614\n",
        "      resnet152,fc,cosine, MMAP 0.776918319273\n",
        "      resnet152,fc,d1, MMAP 0.70010267663\n",
        "'''\n",
        "\n",
        "use_gpu = torch.cuda.is_available()\n",
        "means = np.array([103.939, 116.779, 123.68]) / 255. # mean of three channels in the order of BGR\n",
        "\n",
        "# cache dir\n",
        "cache_dir = 'cache'\n",
        "if not os.path.exists(cache_dir):\n",
        "  os.makedirs(cache_dir)\n",
        "\n",
        "\n",
        "# from https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n",
        "model_urls = {\n",
        "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
        "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
        "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
        "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
        "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
        "}\n",
        "\n",
        "class ResidualNet(ResNet):\n",
        "  def __init__(self, model=RES_model, pretrained=True):\n",
        "    if model == \"resnet18\":\n",
        "        super().__init__(BasicBlock, [2, 2, 2, 2], 1000)\n",
        "        if pretrained:\n",
        "            self.load_state_dict(model_zoo.load_url(model_urls['resnet18']))\n",
        "    elif model == \"resnet34\":\n",
        "        super().__init__(BasicBlock, [3, 4, 6, 3], 1000)\n",
        "        if pretrained:\n",
        "            self.load_state_dict(model_zoo.load_url(model_urls['resnet34']))\n",
        "    elif model == \"resnet50\":\n",
        "        super().__init__(Bottleneck, [3, 4, 6, 3], 1000)\n",
        "        if pretrained:\n",
        "            self.load_state_dict(model_zoo.load_url(model_urls['resnet50']))\n",
        "    elif model == \"resnet101\":\n",
        "        super().__init__(Bottleneck, [3, 4, 23, 3], 1000)\n",
        "        if pretrained:\n",
        "            self.load_state_dict(model_zoo.load_url(model_urls['resnet101']))\n",
        "    elif model == \"resnet152\":\n",
        "        super().__init__(Bottleneck, [3, 8, 36, 3], 1000)\n",
        "        if pretrained:\n",
        "            self.load_state_dict(model_zoo.load_url(model_urls['resnet152']))\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.bn1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.maxpool(x)\n",
        "    x = self.layer1(x)\n",
        "    x = self.layer2(x)\n",
        "    x = self.layer3(x)\n",
        "    x = self.layer4(x)  # x after layer4, shape = N * 512 * H/32 * W/32\n",
        "    max_pool = torch.nn.MaxPool2d((x.size(-2),x.size(-1)), stride=(x.size(-2),x.size(-1)), padding=0, ceil_mode=False)\n",
        "    Max = max_pool(x)  # avg.size = N * 512 * 1 * 1\n",
        "    Max = Max.view(Max.size(0), -1)  # avg.size = N * 512\n",
        "    avg_pool = torch.nn.AvgPool2d((x.size(-2),x.size(-1)), stride=(x.size(-2),x.size(-1)), padding=0, ceil_mode=False, count_include_pad=True)\n",
        "    avg = avg_pool(x)  # avg.size = N * 512 * 1 * 1\n",
        "    avg = avg.view(avg.size(0), -1)  # avg.size = N * 512\n",
        "    fc = self.fc(avg)  # fc.size = N * 1000\n",
        "    output = {\n",
        "        'max': Max,\n",
        "        'avg': avg,\n",
        "        'fc' : fc\n",
        "    }\n",
        "    return output\n",
        "\n",
        "\n",
        "class ResNetFeat(object):\n",
        "\n",
        "  def make_samples(self, db, verbose=True):\n",
        "    sample_cache = '{}-{}'.format(RES_model, pick_layer)\n",
        "  \n",
        "    try:\n",
        "      samples = cPickle.load(open(os.path.join(cache_dir, sample_cache), \"rb\", True))\n",
        "      for sample in samples:\n",
        "        sample['hist'] /= np.sum(sample['hist'])  # normalize\n",
        "      if verbose:\n",
        "        print(\"Using cache..., config=%s, distance=%s, depth=%s\" % (sample_cache, d_type, depth))\n",
        "    except:\n",
        "      if verbose:\n",
        "        print(\"Counting histogram..., config=%s, distance=%s, depth=%s\" % (sample_cache, d_type, depth))\n",
        "  \n",
        "      res_model = ResidualNet(model=RES_model)\n",
        "      res_model.eval()\n",
        "      if use_gpu:\n",
        "        res_model = res_model.cuda()\n",
        "      samples = []\n",
        "      data = db.get_data()\n",
        "      for d in data.itertuples():\n",
        "        d_img, d_cls = getattr(d, \"img\"), getattr(d, \"cls\")\n",
        "        img = scipy.misc.imread(d_img, mode=\"RGB\")\n",
        "        img = img[:, :, ::-1]  # switch to BGR\n",
        "        img = np.transpose(img, (2, 0, 1)) / 255.\n",
        "        img[0] -= means[0]  # reduce B's mean\n",
        "        img[1] -= means[1]  # reduce G's mean\n",
        "        img[2] -= means[2]  # reduce R's mean\n",
        "        img = np.expand_dims(img, axis=0)\n",
        "        try:\n",
        "          if use_gpu:\n",
        "            inputs = torch.autograd.Variable(torch.from_numpy(img).cuda().float())\n",
        "          else:\n",
        "            inputs = torch.autograd.Variable(torch.from_numpy(img).float())\n",
        "          d_hist = res_model(inputs)[pick_layer]\n",
        "          d_hist = d_hist.data.cpu().numpy().flatten()\n",
        "          d_hist /= np.sum(d_hist)  # normalize\n",
        "          samples.append({\n",
        "                          'img':  d_img, \n",
        "                          'cls':  d_cls, \n",
        "                          'hist': d_hist\n",
        "                         })\n",
        "        except:\n",
        "          pass\n",
        "      cPickle.dump(samples, open(os.path.join(cache_dir, sample_cache), \"wb\", True))\n",
        "  \n",
        "    return samples\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  # evaluate database\n",
        "  db = Database()\n",
        "  APs = evaluate_class(db, f_class=ResNetFeat, d_type=d_type, depth=depth)\n",
        "  cls_MAPs = []\n",
        "  for cls, cls_APs in APs.items():\n",
        "    MAP = np.mean(cls_APs)\n",
        "    print(\"Class {}, MAP {}\".format(cls, MAP))\n",
        "    cls_MAPs.append(MAP)\n",
        "  print(\"MMAP\", np.mean(cls_MAPs))\n"
      ],
      "metadata": {
        "id": "abg3z-irHlwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "from torchvision.models.vgg import VGG\n",
        "\n",
        "from six.moves import cPickle\n",
        "import numpy as np\n",
        "import scipy.misc\n",
        "import os\n",
        "\n",
        "from evaluate import evaluate_class\n",
        "from DB import Database\n",
        "\n",
        "\n",
        "'''\n",
        "  downloading problem in mac OSX should refer to this answer:\n",
        "    https://stackoverflow.com/a/42334357\n",
        "'''\n",
        "\n",
        "# configs for histogram\n",
        "VGG_model  = 'vgg19'  # model type\n",
        "pick_layer = 'avg'    # extract feature of this layer\n",
        "d_type     = 'd1'     # distance type\n",
        "\n",
        "depth      = 3        # retrieved depth, set to None will count the ap for whole database\n",
        "\n",
        "''' MMAP\n",
        "     depth\n",
        "      depthNone, vgg19,avg,d1, MMAP 0.688624709114\n",
        "      depth100,  vgg19,avg,d1, MMAP 0.754443491363\n",
        "      depth30,   vgg19,avg,d1, MMAP 0.838298388513\n",
        "      depth10,   vgg19,avg,d1, MMAP 0.913892057193\n",
        "      depth5,    vgg19,avg,d1, MMAP 0.936158333333\n",
        "      depth3,    vgg19,avg,d1, MMAP 0.941666666667\n",
        "      depth1,    vgg19,avg,d1, MMAP 0.934\n",
        "\n",
        "      (exps below use depth=None)\n",
        "\n",
        "      vgg19,fc1,d1, MMAP 0.245548035893 (w/o subtract mean)\n",
        "      vgg19,fc1,d1, MMAP 0.332583126964\n",
        "      vgg19,fc1,co, MMAP 0.333836506148\n",
        "      vgg19,fc2,d1, MMAP 0.294452201395\n",
        "      vgg19,fc2,co, MMAP 0.297209571796\n",
        "      vgg19,avg,d1, MMAP 0.688624709114\n",
        "      vgg19,avg,co, MMAP 0.674217021273\n",
        "'''\n",
        "\n",
        "use_gpu = torch.cuda.is_available()\n",
        "means = np.array([103.939, 116.779, 123.68]) / 255. # mean of three channels in the order of BGR\n",
        "\n",
        "# cache dir\n",
        "cache_dir = 'cache'\n",
        "if not os.path.exists(cache_dir):\n",
        "  os.makedirs(cache_dir)\n",
        "\n",
        "\n",
        "class VGGNet(VGG):\n",
        "  def __init__(self, pretrained=True, model='vgg16', requires_grad=False, remove_fc=False, show_params=False):\n",
        "    super().__init__(make_layers(cfg[model]))\n",
        "    self.ranges = ranges[model]\n",
        "    self.fc_ranges = ((0, 2), (2, 5), (5, 7))\n",
        "\n",
        "    if pretrained:\n",
        "      exec(\"self.load_state_dict(models.%s(pretrained=True).state_dict())\" % model)\n",
        "\n",
        "    if not requires_grad:\n",
        "      for param in super().parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    if remove_fc:  # delete redundant fully-connected layer params, can save memory\n",
        "      del self.classifier\n",
        "\n",
        "    if show_params:\n",
        "      for name, param in self.named_parameters():\n",
        "        print(name, param.size())\n",
        "\n",
        "  def forward(self, x):\n",
        "    output = {}\n",
        "\n",
        "    x = self.features(x)\n",
        "\n",
        "    avg_pool = torch.nn.AvgPool2d((x.size(-2), x.size(-1)), stride=(x.size(-2), x.size(-1)), padding=0, ceil_mode=False, count_include_pad=True)\n",
        "    avg = avg_pool(x)  # avg.size = N * 512 * 1 * 1\n",
        "    avg = avg.view(avg.size(0), -1)  # avg.size = N * 512\n",
        "    output['avg'] = avg\n",
        "\n",
        "    x = x.view(x.size(0), -1)  # flatten()\n",
        "    dims = x.size(1)\n",
        "    if dims >= 25088:\n",
        "      x = x[:, :25088]\n",
        "      for idx in range(len(self.fc_ranges)):\n",
        "        for layer in range(self.fc_ranges[idx][0], self.fc_ranges[idx][1]):\n",
        "          x = self.classifier[layer](x)\n",
        "        output[\"fc%d\"%(idx+1)] = x\n",
        "    else:\n",
        "      w = self.classifier[0].weight[:, :dims]\n",
        "      b = self.classifier[0].bias\n",
        "      x = torch.matmul(x, w.t()) + b\n",
        "      x = self.classifier[1](x)\n",
        "      output[\"fc1\"] = x\n",
        "      for idx in range(1, len(self.fc_ranges)):\n",
        "        for layer in range(self.fc_ranges[idx][0], self.fc_ranges[idx][1]):\n",
        "          x = self.classifier[layer](x)\n",
        "        output[\"fc%d\"%(idx+1)] = x\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "ranges = {\n",
        "  'vgg11': ((0, 3), (3, 6),  (6, 11),  (11, 16), (16, 21)),\n",
        "  'vgg13': ((0, 5), (5, 10), (10, 15), (15, 20), (20, 25)),\n",
        "  'vgg16': ((0, 5), (5, 10), (10, 17), (17, 24), (24, 31)),\n",
        "  'vgg19': ((0, 5), (5, 10), (10, 19), (19, 28), (28, 37))\n",
        "}\n",
        "\n",
        "# cropped version from https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py\n",
        "cfg = {\n",
        "  'vgg11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "  'vgg13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "  'vgg16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "  'vgg19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "}\n",
        "\n",
        "def make_layers(cfg, batch_norm=False):\n",
        "  layers = []\n",
        "  in_channels = 3\n",
        "  for v in cfg:\n",
        "    if v == 'M':\n",
        "      layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "    else:\n",
        "      conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
        "      if batch_norm:\n",
        "        layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
        "      else:\n",
        "        layers += [conv2d, nn.ReLU(inplace=True)]\n",
        "      in_channels = v\n",
        "  return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "class VGGNetFeat(object):\n",
        "\n",
        "  def make_samples(self, db, verbose=True):\n",
        "    sample_cache = '{}-{}'.format(VGG_model, pick_layer)\n",
        "  \n",
        "    try:\n",
        "      samples = cPickle.load(open(os.path.join(cache_dir, sample_cache), \"rb\", True))\n",
        "      for sample in samples:\n",
        "        sample['hist'] /= np.sum(sample['hist'])  # normalize\n",
        "      cPickle.dump(samples, open(os.path.join(cache_dir, sample_cache), \"wb\", True))\n",
        "      if verbose:\n",
        "        print(\"Using cache..., config=%s, distance=%s, depth=%s\" % (sample_cache, d_type, depth))\n",
        "    except:\n",
        "      if verbose:\n",
        "        print(\"Counting histogram..., config=%s, distance=%s, depth=%s\" % (sample_cache, d_type, depth))\n",
        "  \n",
        "      vgg_model = VGGNet(requires_grad=False, model=VGG_model)\n",
        "      vgg_model.eval()\n",
        "      if use_gpu:\n",
        "        vgg_model = vgg_model.cuda()\n",
        "      samples = []\n",
        "      data = db.get_data()\n",
        "      for d in data.itertuples():\n",
        "        d_img, d_cls = getattr(d, \"img\"), getattr(d, \"cls\")\n",
        "        img = scipy.misc.imread(d_img, mode=\"RGB\")\n",
        "        img = img[:, :, ::-1]  # switch to BGR\n",
        "        img = np.transpose(img, (2, 0, 1)) / 255.\n",
        "        img[0] -= means[0]  # reduce B's mean\n",
        "        img[1] -= means[1]  # reduce G's mean\n",
        "        img[2] -= means[2]  # reduce R's mean\n",
        "        img = np.expand_dims(img, axis=0)\n",
        "        try:\n",
        "          if use_gpu:\n",
        "            inputs = torch.autograd.Variable(torch.from_numpy(img).cuda().float())\n",
        "          else:\n",
        "            inputs = torch.autograd.Variable(torch.from_numpy(img).float())\n",
        "          d_hist = vgg_model(inputs)[pick_layer]\n",
        "          d_hist = np.sum(d_hist.data.cpu().numpy(), axis=0)\n",
        "          d_hist /= np.sum(d_hist)  # normalize\n",
        "          samples.append({\n",
        "                          'img':  d_img, \n",
        "                          'cls':  d_cls, \n",
        "                          'hist': d_hist\n",
        "                         })\n",
        "        except:\n",
        "          pass\n",
        "      cPickle.dump(samples, open(os.path.join(cache_dir, sample_cache), \"wb\", True))\n",
        "  \n",
        "    return samples\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  # evaluate database\n",
        "  db = Database()\n",
        "  APs = evaluate_class(db, f_class=VGGNetFeat, d_type=d_type, depth=depth)\n",
        "  cls_MAPs = []\n",
        "  for cls, cls_APs in APs.items():\n",
        "    MAP = np.mean(cls_APs)\n",
        "    print(\"Class {}, MAP {}\".format(cls, MAP))\n",
        "    cls_MAPs.append(MAP)\n",
        "  print(\"MMAP\", np.mean(cls_MAPs))\n"
      ],
      "metadata": {
        "id": "XEnLp0L9H9zX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}